/**
 * =====================================================
 * app.js
 * =====================================================
 * Frontend WebRTC para OpenAI Realtime (audio real)
 * - Captura micrÃ³fono
 * - Reproduce audio remoto del asistente
 * - Fuerza saludo hablado al iniciar llamada
 *
 * CLAVE:
 * - NO usar response.output_modalities (te da Unknown parameter)
 * - Usar response.modalities
 * =====================================================
 */

import { VOICE_CONFIG } from "./voice-config.js";
import { getGreetingByTime } from "./utils/greeting.js";

console.log("app.js cargado");

const startBtn = document.getElementById("startBtn");
const stopBtn = document.getElementById("stopBtn");
const statusEl = document.getElementById("status");
const logEl = document.getElementById("log");

let pc = null;
let dc = null;
let localStream = null;
let remoteAudio = null;

function log(msg) {
  const line = `[${new Date().toLocaleTimeString()}] ${msg}`;
  logEl.textContent += line + "\n";
  logEl.scrollTop = logEl.scrollHeight;
  console.log(line);
}

function sendEvent(payload) {
  if (dc && dc.readyState === "open") {
    dc.send(JSON.stringify(payload));
  }
}

async function startCall() {
  try {
    startBtn.disabled = true;
    stopBtn.disabled = false;
    statusEl.textContent = "Conectandoâ€¦";

    log("ðŸ“ž Llamar pulsado");

    // 1) PeerConnection
    pc = new RTCPeerConnection();

    // 2) Audio remoto
    remoteAudio = document.createElement("audio");
    remoteAudio.autoplay = true;
    remoteAudio.playsInline = true;
    remoteAudio.muted = false;
    remoteAudio.volume = 1;
    document.body.appendChild(remoteAudio);

    pc.ontrack = async (e) => {
      log("ðŸ”Š Audio remoto recibido (track)");
      remoteAudio.srcObject = e.streams[0];

      // Intentar play() (gesture ya ocurriÃ³ al pulsar "Llamar")
      try {
        await remoteAudio.play();
        log("âœ… remoteAudio.play() OK");
      } catch (err) {
        log("âš ï¸ remoteAudio.play() bloqueado: " + (err?.message || err));
      }
    };

    // 3) MicrÃ³fono
    localStream = await navigator.mediaDevices.getUserMedia({ audio: true });
    localStream.getTracks().forEach((t) => pc.addTrack(t, localStream));
    log("ðŸŽ¤ MicrÃ³fono capturado");

    // 4) DataChannel
    dc = pc.createDataChannel("oai-events");

    dc.onopen = () => {
      log("ðŸŸ¢ DataChannel abierto");

      const greeting = getGreetingByTime();
      const systemPrompt = VOICE_CONFIG.buildSystemPrompt({ greeting });

      // A) Ajustar sesiÃ³n (instrucciones)
      // Nota: aquÃ­ NO meto output_modalities para evitar sorpresas;
      // el audio ya viene por la pista remota si la sesiÃ³n fue creada con voz.
      sendEvent({
        type: "session.update",
        session: {
          instructions: systemPrompt,
        },
      });

      // B) Crear item "user" que ordena el saludo EXACTO
      sendEvent({
        type: "conversation.item.create",
        item: {
          type: "message",
          role: "user",
          content: [
            {
              type: "input_text",
              text:
                `Di exactamente: "${greeting}, ${VOICE_CONFIG.clinicName}, le atiende ${VOICE_CONFIG.assistantName}." ` +
                `Luego quÃ©date en silencio y espera.`,
            },
          ],
        },
      });

      // C) Pedir respuesta en AUDIO (âœ… usar modalities, no output_modalities)
      sendEvent({
        type: "response.create",
        response: {
          modalities: ["audio", "text"],
          // opcional: si quieres forzar un poco mÃ¡s:
          // instructions: systemPrompt,
        },
      });

      log("ðŸ“¢ Saludo solicitado al asistente (AUDIO)");
    };

    dc.onmessage = (event) => {
      try {
        const data = JSON.parse(event.data);
        if (data?.type) log(`ðŸ“© Event: ${data.type}`);

        // Si quieres ver el error exacto cuando haya "Event: error"
        if (data?.type === "error") {
          log("âŒ OpenAI error payload: " + JSON.stringify(data, null, 2));
        }
      } catch {
        // ignore
      }
    };

    // 5) SDP Offer -> backend /session -> SDP Answer
    const offer = await pc.createOffer();
    await pc.setLocalDescription(offer);

    log("ðŸ“¡ Enviando SDP a /sessionâ€¦");
    const sdpRes = await fetch("/session", {
      method: "POST",
      headers: { "Content-Type": "application/sdp" },
      body: offer.sdp,
    });

    if (!sdpRes.ok) {
      const err = await sdpRes.text();
      throw new Error("Error /session: " + err);
    }

    const answerSdp = await sdpRes.text();
    await pc.setRemoteDescription({ type: "answer", sdp: answerSdp });

    statusEl.textContent = "En llamadaâ€¦";
    log("âœ… Llamada establecida");
  } catch (e) {
    log("âŒ Error: " + (e?.message || e));
    statusEl.textContent = "Error";
    stopCall();
  }
}

function stopCall() {
  log("ðŸ›‘ Colgar pulsado");

  startBtn.disabled = false;
  stopBtn.disabled = true;

  try {
    dc?.close();
  } catch {}
  try {
    pc?.close();
  } catch {}

  localStream?.getTracks().forEach((t) => t.stop());

  if (remoteAudio) {
    remoteAudio.srcObject = null;
    remoteAudio.remove();
  }

  pc = null;
  dc = null;
  localStream = null;
  remoteAudio = null;

  statusEl.textContent = "Listo.";
  log("ðŸ”´ Llamada finalizada");
}

startBtn.onclick = startCall;
stopBtn.onclick = stopCall;
